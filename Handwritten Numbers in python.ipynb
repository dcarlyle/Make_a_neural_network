{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on one of the projects defined by Andrew Ng and the online course Macine Learning at coursera! That Project has been done in Octave! And I would like to do it with python (Jupyter) to learn more about python and NN. It is written for python 3.\n",
    "\n",
    "There are still a lot of improvement possible like:\n",
    "0. provid numerical partial derivation calculation to check whether the Back Propagation is working fine\n",
    "0. ploting the cost function in terms of iterations\n",
    "0. using advanced optimization algorithm which can choose the learning rate automatically for faster convergence\n",
    "0. checking the learning curves\n",
    "0. changing the ragularization\n",
    "0. dividing data to three parts such as training set, cross validation set and test set and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import scipy\n",
    "import scipy.io as sio           # to import .mat file, data of images\n",
    "import numpy as np\n",
    "import random                    # to create random matrices for weights of NN as initilisation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # to plot the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data from the .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "\n",
      " input (of training data) of the neural netwok\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "size = (5000, 400)\n",
      "\n",
      " output (of training data) of the neural netwok\n",
      "[[10]\n",
      " [10]\n",
      " [10]\n",
      " ..., \n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]]\n",
      "size = (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# import .mat file (octave or matlab file) and show some of them and their size\n",
    "training_data = sio.loadmat('ex4data1.mat')\n",
    "print(type(training_data))\n",
    "# print(training_data)\n",
    "X = training_data['X']\n",
    "y = training_data['y']\n",
    "print('\\n','input (of training data) of the neural netwok')\n",
    "print(X)\n",
    "print('size =',np.shape(X))\n",
    "print('\\n','output (of training data) of the neural netwok')\n",
    "print(y)\n",
    "print('size =',np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 984 is number [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7b155ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show an image\n",
    "# random.seed(1)\n",
    "index = random.randint(0,np.shape(X)[0]) # 0 to 4999 indices\n",
    "print('image' ,index, 'is number', y[index])\n",
    "img = X[index][:]\n",
    "\n",
    "# the data of an image stored in a row and should be reshaped to be shown!\n",
    "# we know that all images are 20 * 20 pixels in size\n",
    "img = img.reshape((20,20)) \n",
    "img = img.T\n",
    "# print(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some description about the below code:\n",
    "\n",
    "theta s are the weight matrices for the input layer and hidden layer.\n",
    "\n",
    "Theta1_grad and Theta2_grad are partial derivatives of cost function with respect to theta s.\n",
    "\n",
    "Line 50 - 57:\n",
    "The data is saved in X and y. X includes images and y includes labels for those images. So raw y is a 5000 by 1 matrix. I wanted to transfer it to a 10 by 5000 matrix. Because if the image is showing number 3, the output should look like this [[0], [0], [1], [0], [0], [0], [0], [0], [0], [0]] as a column-wise vector. Now I need 1 column like this for each image. Then it would be 10 by 5000 as y_vect. So I used y_vect on the rest of the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# given by Siraj: https://github.com/yosoufe/Make_a_neural_network/blob/master/demo.py\n",
    "# and modified by me! the modification is to make the neural network bigger to :\n",
    "# 1 input layer\n",
    "# 1 hidden layer\n",
    "# 1 output layer (with 10 classes, one for each number)\n",
    "\n",
    "\n",
    "from numpy import exp, array, random, dot\n",
    "\n",
    "class NeuralNetwork():\n",
    "    # initilising the random matrices based on dimentsion, mean = zero and range (-0.12,0.12)\n",
    "    def __rand_init_weights(self,L_out, L_in):\n",
    "        init_epsilon = 0.12;\n",
    "        return np.random.random((L_out,L_in)) * (2 * init_epsilon) - init_epsilon;\n",
    "    \n",
    "    # initilise the object with number of neurons at each layer\n",
    "    # if you have some initial weights, you can feed them into the object (as np.ndarray).\n",
    "    # otherwise feed zero as init_theta1,init_theta2\n",
    "    def __init__(self,N_input,N_hidden,N_output,init_theta1,init_theta2):\n",
    "        self.n_input = N_input # #neurons at input level\n",
    "        self.n_hidden = N_hidden # #neurons at hidden level\n",
    "        self.n_output = N_output # #neurons at output level\n",
    "        # this was quite importnant! without good initialisation you will get stuck in local minimum and \n",
    "        # you won't get a cost function better than 3.4\n",
    "        if (isinstance(init_theta1,np.ndarray) and isinstance(init_theta2,np.ndarray)):\n",
    "            if (init_theta1.shape == (N_hidden,N_input+1) and init_theta2.shape == (N_output,N_hidden+1)): # if there is initial value for thetas\n",
    "                self.theta1 = init_theta1\n",
    "                self.theta2 = init_theta2\n",
    "                print('initial weights are consistent with the layers neuron numbers :)' )\n",
    "        else: # if there is no initial theta value for thetas\n",
    "            self.theta1 = self.__rand_init_weights(N_hidden,N_input+1) # initilise weights for first layer (with bias)\n",
    "            self.theta2 = self.__rand_init_weights(N_output,N_hidden+1) # initilise weights for 2nd layer (with bias)\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return scipy.special.expit(x) # can handles exp when is going to be too big (avoid exp overflow)\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        x = self.__sigmoid(x)\n",
    "        return (x * (1.0 - x))\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations,alpha,lamb):\n",
    "        X = training_set_inputs\n",
    "        y = training_set_outputs\n",
    "        m = X.shape[0]; # number of samples\n",
    "        n = X.shape[1]; # number of features\n",
    "        print (m , 'samples with' , n , 'features each')\n",
    "        y_vect = np.zeros((self.n_output,m))\n",
    "        for index in range(m):\n",
    "            y_vect [y[index]-1,index] = 1\n",
    "        \n",
    "        X1 = np.concatenate( (np.ones(( m , 1 )) , training_set_inputs ) , 1)\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            z2 = dot( self.theta1 , X1.T )\n",
    "            a2 = self.__sigmoid(z2)\n",
    "            a2_1 = np.concatenate( (np.ones(( 1 ,a2.shape[1]   )) , a2 ) , 0)\n",
    "            z3 = dot( self.theta2 ,  a2_1)\n",
    "            a3 = self.__sigmoid(z3)  # output of NN\n",
    "\n",
    "            # Cost Function\n",
    "            J = -y_vect * np.log(a3) - (1 - y_vect) * np.log( 1 - a3)\n",
    "            J = J.sum()/m\n",
    "            J = J + lamb/m/2 * (self.theta1.sum() + self.theta2.sum()) # regularization terms\n",
    "            print('\\r','Cost function at iteration',iteration+1,'of',number_of_training_iterations, 'is' ,J, 'with alpha =',alpha, '& lambda =' , lamb, end='')\n",
    "\n",
    "            # Back Propagation\n",
    "            Delta1 = np.zeros(self.theta1.shape)\n",
    "            Delta2 = np.zeros(self.theta2.shape)\n",
    "            index = 1\n",
    "            for index in range(m):\n",
    "                delta3 = a3[:,index]-y_vect[:,index]\n",
    "                delta2 = dot( self.theta2.T , delta3[:,None])[1:] * self.__sigmoid_derivative(z2[:,index])[:,None]\n",
    "                Delta1 = Delta1 + dot(delta2 , X1[index,:][None,:])\n",
    "                Delta2 = Delta2 + dot(delta3[:,None] , a2_1[:,index][None,:] )\n",
    "\n",
    "            Theta1_grad = Delta1 / m\n",
    "            Theta2_grad = Delta2 / m\n",
    "            \n",
    "            # regularized BP\n",
    "            Theta1_grad = Theta1_grad + np.concatenate((np.zeros((self.n_hidden,1)), self.theta1[:,1:]*(lamb/m)),1);\n",
    "            Theta2_grad = Theta2_grad + np.concatenate((np.zeros((self.n_output,1)), self.theta2[:,1:]*(lamb/m)),1);\n",
    "            \n",
    "            adjust_theta1 = Theta1_grad * alpha\n",
    "            adjust_theta2 = Theta2_grad * alpha\n",
    "            \n",
    "            self.theta1 -= adjust_theta1\n",
    "            self.theta2 -= adjust_theta2\n",
    "\n",
    "            \n",
    "    # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "    # Pass inputs through our neural network (our single neuron).\n",
    "        if len(inputs.shape)==1: # only one example\n",
    "            inputs1 = np.concatenate( (np.array([1]) , inputs ) , 0)\n",
    "            inputs1 = inputs1[None,:]\n",
    "            print(inputs1.shape)\n",
    "        else: #more than one example\n",
    "            m = inputs.shape[0]\n",
    "            inputs1 = np.concatenate( (np.ones(( m , 1 )) , inputs ) , 1)\n",
    "        \n",
    "        z2 = dot( self.theta1 , inputs1.T )\n",
    "        a2 = self.__sigmoid(z2)\n",
    "        a2_1 = np.concatenate( (np.ones(( 1 ,a2.shape[1]   )) , a2 ) , 0)\n",
    "        z3 = dot( self.theta2 ,  a2_1)\n",
    "        a3 = self.__sigmoid(z3)  # output of NN\n",
    "        return a3.argmax(0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate an object from neural network class\n",
    "# why 25 hidden neurons? I really do not know! It seems it is comming from experiments\n",
    "# 400 neurons as input bacause images are 20 by 20 in pixels and in total 400 pixels\n",
    "# 10 neurons for output layer because there are 10 different class as output, each for one number [1 to 10]\n",
    "neural_network = NeuralNetwork(400,25,10,0,0) # run this for first time\n",
    "\n",
    "# you can run this with a given theta\n",
    "# neural_network = NeuralNetwork(400,25,10,neural_network.theta1,neural_network.theta2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 samples with 400 features each\n",
      " Cost function at iteration 1000 of 1000 is 0.337136564789 with alpha = 0.9 lambda = 0"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "neural_network.train(X,y,1000,0.9,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 401)\n",
      "Predicted output = [6] and actual output is = [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD9xJREFUeJzt3X+MVeWdx/HPhwFCHDDosqACLphMSNjazjbKNl12g9vW\nAFFpN02FbLauawI2tdkmuzHsblKb6B9NNq5JxaC4JWpitW12aYlQXTQm1KQoqBSl/mCW0MBIYdv6\no2IFZ+a7f9yDGcf7wHPvub95vxJyzz3ne+957szkwzn3nvt8HRECgGomtXsAADoXAQEgiYAAkERA\nAEgiIAAkERAAkggIAEkEBIAkAgJA0uR2D6Aa2zFpEtkFNMvY2Jgiwmer68iAmDRpkvr7+9s9DKBn\nnThxIquu1H/Ttpfbfs32kO31Vbbb9neL7ftsf7rM/gC0Vt0BYbtP0j2SVkhaLGmN7cUTylZIGij+\nrZW0sd79AWi9MkcQSyQNRcTBiDgl6VFJqybUrJL0UFTskjTT9sUl9gmghcoExFxJh8fdP1Ksq7UG\nQIfqmDcpba9V5TRE9lnfXAXQAmUCYljS/HH35xXraq2RJEXEJkmbJKmvr49ZbIAOUOYUY7ekAdsL\nbU+VtFrS1gk1WyV9tfg04zOS3o6IoyX2CaCF6j6CiIgR27dIekJSn6TNEbHf9s3F9nslbZe0UtKQ\npPck3Vh+yABaxZ04J2VfX19woRTQPCdOnNDo6Gh3XkmJ3jYyMpJdW8sb1n19ffUMB2fAFx4AJBEQ\nAJIICABJBASAJAICQBIBASCJgACQREAASCIgACQREACSuNQaSbV8T2dsbCy7dt26ddm1c+fmzy90\n2223ZddyWXYejiAAJBEQAJIICABJBASAJAICQBIBASCpTGet+baftv1L2/tt/2OVmmW237a9t/j3\nrXLDBdBKZa6DGJH0TxHxgu0Zkp63vSMifjmh7mcRcU2J/QBok7qPICLiaES8UCz/XtIromsW0FMa\n8h6E7QWS/kzSs1U2f7bo7P1T23/aiP0BaI3Sl1rbni7pvyR9MyLembD5BUmXRsS7tldK+rEqnb6r\nPQ+t9zrM6Ohodu11112XXXvzzTdn127cmN8QvhNbOHS7UkcQtqeoEg4PR8R/T9weEe9ExLvF8nZJ\nU2zPqvZcEbEpIq6IiCsICKAzlPkUw5K+J+mViPiPRM1FRZ1sLyn299t69wmgtcqcYvyFpL+T9JLt\nvcW6f5V0qfRh670vS/qa7RFJf5C0OjgOBLpGmd6cz0g647lARGyQtKHefQBoL66kBJBEQABIIiAA\nJBEQAJIICABJBASAJGa1PsfUchnK9OnTs2truXx627Zt2bWbNm3KrmWm6sbjCAJAEgEBIImAAJBE\nQABIIiAAJBEQAJIICABJBASAJAICQBJXUp5jTp48mV176623ZtfOnj07u/b+++/Prq3lyk/mMm08\njiAAJJWd1fqQ7ZeKtnp7qmy37e/aHip6Y3y6zP4AtFYjTjGuiojfJLatUKUPxoCkP5e0sbgF0AWa\nfYqxStJDUbFL0kzbFzd5nwAapGxAhKQnbT9fdMaaaK6kw+PuHxH9O4GuUfYUY2lEDNueLWmH7Vcj\nYmc9T0TrPaDzlDqCiIjh4va4pC2SlkwoGZY0f9z9ecW6as9F6z2gw5Rpvddve8bpZUlXS3p5QtlW\nSV8tPs34jKS3I+Jo3aMF0FJlTjHmSNpS/G8/WdL3I+Jx2zdLH7be2y5ppaQhSe9JurHccAG0UpnW\newclfarK+nvHLYekr9e7DwDtxaXWPaBZE9FeeeWV2bWPPfZYdu3hw4fPXlSYMmVKdi0aj0utASQR\nEACSCAgASQQEgCQCAkASAQEgiYAAkERAAEgiIAAkERAAkrjUugfUMlP19ddfn127dOnS7NrNmzdn\n146OjmbXTprE/2HtxE8fQBIBASCJgACQREAASCIgACQREACSCAgASWVmtV5U9OQ8/e8d29+cULPM\n9tvjar5VfsgAWqXMpLWvSRqUJNt9qvS72FKl9GcRcU29+wHQPo06xficpP+NiF816PkAdIBGXWq9\nWtIjiW2ftb1PlSOMf46I/dWKaL1Xv1pmtb7ooouya2uZAbuWmaq5fLp7lP5N2Z4q6TpJP6qy+QVJ\nl0bEJyXdLenHqeeh9R7QeRoR5SskvRARxyZuiIh3IuLdYnm7pCm2ZzVgnwBaoBEBsUaJ0wvbF7k4\nHLC9pNjfbxuwTwAtUOo9iKJp7xckrRu3bnxvzi9L+prtEUl/kLQ6ajlhBtBWpQIiIk5I+qMJ68b3\n5twgaUOZfQBoH95OBpBEQABIIiAAJBEQAJIICABJzGrdoWr5NLi/vz+79rLLLsuu3b17d3bt/v1V\nr6BHl+MIAkASAQEgiYAAkERAAEgiIAAkERAAkggIAEkEBIAkAgJAEgEBIIlLrTvU2NhYdu2FF16Y\nXbtkyZLs2vvuuy+79oMPPsiunTFjRnbte++9l13LbNmNx08UQNJZA8L2ZtvHbb88bt2FtnfYPlDc\nXpB47HLbr9kesr2+kQMH0Hw5RxAPSFo+Yd16SU9FxICkp4r7H1G047tHlWnxF0taY3txqdECaKmz\nBkRE7JT0uwmrV0l6sFh+UNIXqzx0iaShiDgYEackPVo8DkCXqPc9iDkRcbRY/rWkOVVq5koa34/t\nSLEOQJco/SlGRITt0r0u6M0JdJ56jyCO2b5Ykorb41VqhiXNH3d/XrGuKnpzAp2n3oDYKumGYvkG\nST+pUrNb0oDthUWD39XF4wB0iZyPOR+R9HNJi2wfsX2TpO9I+oLtA5I+X9yX7Utsb5ekiBiRdIuk\nJyS9IumHEcHEhUAXOet7EBGxJrHpc1Vq35C0ctz97ZK21z06AG3FpdYdqpZZradPn55du2DBguza\n2bNnZ9fefffd2bUzZ87Mrr399tuzaw8cOJBdO3kyf/o5uNQaQBIBASCJgACQREAASCIgACQREACS\nCAgASQQEgCQCAkASAQEgietNO1QtX3mvZebnnTt3Zte+/vrr2bX79u3Lrq1lZu077rgju/bGG2/M\nrj158mRW3bk+9QBHEACSCAgASQQEgCQCAkASAQEgiYAAkFRv671/t/2q7X22t9iuOkWQ7UO2X7K9\n1/aeRg4cQPPV23pvh6RPRMQnJb0u6V/O8PirImIwIq6ob4gA2qWu1nsR8T/FrNWStEuVnhcAekwj\n3oP4B0k/TWwLSU/afr7onAWgi5S61Nr2v0kakfRwomRpRAzbni1ph+1XiyOSas9F671xapnVetq0\nadm1559/fnbt0NBQdu2zzz6bXfv0009n1z78cOpP6+MWLVqUXfviiy9m1Z3rs1/XfQRh++8lXSPp\nbyPx1xwRw8XtcUlbVOn4XRWt94DOU1dA2F4u6VZJ10VE1W8K2e63PeP0sqSrJb1crRZAZ6q39d4G\nSTNUOW3Ya/veovbD1nuS5kh6xvYvJD0naVtEPN6UVwGgKeptvfe9RO2Hrfci4qCkT5UaHYC24kpK\nAEkEBIAkAgJAEgEBIImAAJBEQABIOrevI+1gkyblZ/dbb72VXXvq1Kns2ssvvzy7dteuXdm1AwMD\n2bXnnXdedu0bb7yRXVvLz/dcxk8JQBIBASCJgACQREAASCIgACQREACSCAgASQQEgCQCAkASV1J2\nqFqu9HvzzTezazdu3Jhdu2LFiuzaefPyOx8sXLgwu/auu+7Krj127Fh27bk+GW0ujiAAJNXbeu/b\ntoeL+Sj32l6ZeOxy26/ZHrK9vpEDB9B89bbek6S7ipZ6gxGxfeJG232S7pG0QtJiSWtsLy4zWACt\nVVfrvUxLJA1FxMGIOCXpUUmr6ngeAG1S5j2IbxTdvTfbvqDK9rmSDo+7f6RYB6BL1BsQGyVdJmlQ\n0lFJd5YdiO21tvfY3lNL2zkAzVNXQETEsYgYjYgxSfereku9YUnzx92fV6xLPSet94AOU2/rvYvH\n3f2SqrfU2y1pwPZC21MlrZa0tZ79AWiPs14tUrTeWyZplu0jkm6TtMz2oKSQdEjSuqL2Ekn/GREr\nI2LE9i2SnpDUJ2lzROxvyqsA0BRNa71X3N8u6WMfgQLoDu7ENwT7+vqiv7+/3cPoSbX8vq+99trs\n2gULFmTXPvfcc9m1tUyGy0S0+U6cOKHR0dGzvtnHTxRAEgEBIImAAJBEQABIIiAAJBEQAJIICABJ\nBASAJAICQBIBASCJS62R9P7772fXjoyMZNdOnTq1KbXIx6XWAEojIAAkERAAkggIAEkEBIAkAgJA\nUs6clJslXSPpeER8olj3A0mLipKZkt6KiMEqjz0k6feSRiWNRMQVDRo3gBbIaXH8gKQNkh46vSIi\nrj+9bPtOSW+f4fFXRcRv6h0ggPbJmbR2p+0F1ba50sDiK5L+urHDAtAJyr4H8ZeSjkXEgcT2kPSk\n7edtry25LwAtlnOKcSZrJD1yhu1LI2LY9mxJO2y/WjQD/pgiQNYWyyWHhUaYNm1au4eANqv7CML2\nZEl/I+kHqZqIGC5uj0vaouot+k7X0noP6DBlTjE+L+nViDhSbaPtftszTi9LulrVW/QB6FBnDYii\n9d7PJS2yfcT2TcWm1ZpwemH7EtunO2nNkfSM7V9Iek7Stoh4vHFDB9BsfN0bOAfxdW8ApREQAJII\nCABJBASAJAICQBIBASCJgACQREAASCIgACQREACSCAgASQQEgCQCAkASAQEgiYAAkERAAEgiIAAk\ndeSMUrb/T9KvJqyeJakXG/D06uuSeve19cLr+pOI+OOzFXVkQFRje08vtu7r1dcl9e5r69XXVQ2n\nGACSCAgASd0UEJvaPYAm6dXXJfXua+vV1/UxXfMeBIDW66YjCAAt1vEBYXu57ddsD9le3+7xNJLt\nQ7Zfsr3X9p52j6detjfbPm775XHrLrS9w/aB4vaCdo6xXonX9m3bw8Xvba/tle0cYzN1dEDY7pN0\nj6QVkhZLWmN7cXtH1XBXRcRgl39s9oCk5RPWrZf0VEQMSHqquN+NHtDHX5sk3VX83gYjYnuV7T2h\nowNClW7gQxFxMCJOSXpU0qo2jwkTRMROSb+bsHqVpAeL5QclfbGlg2qQxGs7Z3R6QMyVdHjc/SPF\nul4Rkp60/bztte0eTIPNiYijxfKvVWnm3Eu+YXtfcQrSladPOTo9IHrd0ogYVOUU6uu2/6rdA2qG\nqHxU1ksfl22UdJmkQUlHJd3Z3uE0T6cHxLCk+ePuzyvW9YSIGC5uj0vaosopVa84ZvtiSSpuj7d5\nPA0TEcciYjQixiTdr976vX1EpwfEbkkDthfanipptaStbR5TQ9jutz3j9LKkqyW9fOZHdZWtkm4o\nlm+Q9JM2jqWhTgdf4Uvqrd/bR0xu9wDOJCJGbN8i6QlJfZI2R8T+Ng+rUeZI2mJbqvwevh8Rj7d3\nSPWx/YikZZJm2T4i6TZJ35H0Q9s3qfLN3K+0b4T1S7y2ZbYHVTltOiRpXdsG2GRcSQkgqdNPMQC0\nEQEBIImAAJBEQABIIiAAJBEQAJIICABJBASApP8HqYh28CW8JgUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7b150d048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check how well it works. You can implement a function to check for all images and calculate the precision!\n",
    "# In octave, the same algorithm reached more than 90% but with using some advenced optimisation algorithms\n",
    "ind = 3260 # change the index to see other images\n",
    "print('Predicted output =' ,neural_network.think(X[ind,:]), 'and actual output is =', y[ind])\n",
    "img = X[ind][:]\n",
    "\n",
    "# the data of an image stored in a row and should be reshaped to be shown!\n",
    "# we know that all images are 20 * 20 pixels in size\n",
    "img = img.reshape((20,20)) \n",
    "img = img.T\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
